# ACP : Analyse en Composantes Principales

## RÃ©sumÃ© de lâ€™ACP
Lâ€™Analyse en Composantes Principales (ACP) est une mÃ©thode statistique utilisÃ©e pour rÃ©duire la dimensionnalitÃ© dâ€™un ensemble de donnÃ©es tout en conservant un maximum dâ€™information. Elle transforme des variables corrÃ©lÃ©es en un nouvel ensemble de variables non corrÃ©lÃ©es, appelÃ©es composantes principales. Ces composantes sont ordonnÃ©es de sorte que la premiÃ¨re explique le plus de variance possible, la seconde le second plus grand montant de variance, etc.  

Lâ€™ACP est souvent utilisÃ©e en exploration de donnÃ©es, en visualisation (notamment quand on a des donnÃ©es multidimensionnelles), et en prÃ©traitement pour des modÃ¨les dâ€™apprentissage automatique.  


## DÃ©tails de lâ€™ACP

1. Objectif  
Lâ€™ACP cherche Ã  rÃ©sumer lâ€™information contenue dans un grand nombre de variables en un plus petit nombre de nouvelles variables, tout en minimisant la perte dâ€™information.  

2. Ã‰tapes du calcul  

    1. Standardisation des donnÃ©es  
       - Comme lâ€™ACP est influencÃ©e par lâ€™Ã©chelle des variables, on commence par centrer et rÃ©duire les donnÃ©es (moyenne 0 et Ã©cart-type 1 pour chaque variable).  
     
    2. Calcul de la matrice de covariance (ou de corrÃ©lation)  
       - Cette matrice indique comment les variables sont liÃ©es entre elles.  
       - Si les variables sont sur des Ã©chelles diffÃ©rentes, on privilÃ©gie la matrice de corrÃ©lation plutÃ´t que celle de covariance.  

    3. DÃ©composition en valeurs propres et vecteurs propres  
       - On calcule les valeurs propres et les vecteurs propres de la matrice de covariance.  
       - Les valeurs propres mesurent la variance expliquÃ©e par chaque composante principale.  
       - Les vecteurs propres (aussi appelÃ©s composantes principales) dÃ©finissent la direction des nouvelles variables.  

    4. SÃ©lection des composantes principales  
       - On choisit les composantes qui expliquent le plus de variance en regardant le cumul des valeurs propres.  
       - Un critÃ¨re courant est de conserver assez de composantes pour expliquer 80-90% de la variance.  
       - On peut aussi utiliser le critÃ¨re du coude, qui consiste Ã  repÃ©rer un point oÃ¹ l'ajout d'une nouvelle composante n'apporte plus beaucoup de gain en variance.  

    5. Projection des donnÃ©es sur les nouvelles composantes  
       - Les donnÃ©es initiales sont transformÃ©es dans ce nouvel espace formÃ© par les composantes principales.  
       - Cela permet une visualisation en 2D ou 3D si on conserve seulement 2 ou 3 composantes.  

 3. Applications courantes de lâ€™ACP  

- Visualisation des donnÃ©es : Par exemple, en biologie, lâ€™ACP est utilisÃ©e pour explorer des groupes dâ€™individus en gÃ©nÃ©tique.  
- Compression de donnÃ©es : Utile dans lâ€™apprentissage automatique pour rÃ©duire le nombre de dimensions et accÃ©lÃ©rer les algorithmes.  
- DÃ©tection dâ€™anomalies : Une observation qui se projette loin des autres sur les premiÃ¨res composantes pourrait Ãªtre une anomalie.  
- PrÃ©traitement en Machine Learning : Lâ€™ACP peut aider Ã  rÃ©duire la colinÃ©aritÃ© entre variables et amÃ©liorer la performance des modÃ¨les prÃ©dictifs.  


## Exemple dÃ©taillÃ© dâ€™ACP

Nous allons appliquer lâ€™ACP sur un jeu de donnÃ©es fictif contenant les caractÃ©ristiques de clients dâ€™une banque. Lâ€™objectif est de rÃ©duire la dimensionnalitÃ© tout en conservant un maximum dâ€™information pour mieux comprendre la structure des donnÃ©es.

 1. Jeu de donnÃ©es  
Imaginons que nous avons une base de 10 clients avec les caractÃ©ristiques suivantes :  

| Client | Revenu annuel (â‚¬) | Score crÃ©dit | Age | Montant Ã©pargne (â‚¬) |
|--------|------------------|-------------|-----|---------------------|
| A      | 35000            | 650         | 25  | 12000               |
| B      | 40000            | 700         | 30  | 15000               |
| C      | 25000            | 500         | 22  | 8000                |
| D      | 60000            | 800         | 45  | 30000               |
| E      | 70000            | 850         | 50  | 40000               |
| F      | 20000            | 480         | 20  | 5000                |
| G      | 55000            | 770         | 40  | 27000               |
| H      | 30000            | 580         | 27  | 11000               |
| I      | 45000            | 720         | 33  | 17000               |
| J      | 65000            | 810         | 47  | 35000               |

Les variables sont corrÃ©lÃ©es :  
- Un revenu plus Ã©levÃ© est souvent liÃ© Ã  une meilleure Ã©pargne et un meilleur score crÃ©dit.  
- Lâ€™Ã¢ge peut influencer la capacitÃ© dâ€™Ã©pargne.  

Nous allons utiliser lâ€™ACP pour rÃ©duire ces 4 variables en 2 composantes principales tout en conservant un maximum dâ€™information.

 2. Ã‰tapes de lâ€™ACP  

 1ï¸âƒ£ Standardisation des donnÃ©es  
Lâ€™ACP est influencÃ©e par lâ€™Ã©chelle des variables, donc nous les centrons et rÃ©duisons (moyenne = 0, Ã©cart-type = 1).  

Formule de standardisation :  
\[
X_{\text{standardisÃ©}} = \frac{X - \text{moyenne}(X)}{\text{Ã©cart-type}(X)}
\]

AprÃ¨s transformation, nous obtenons une nouvelle table oÃ¹ chaque variable a une moyenne de 0 et un Ã©cart-type de 1.

 2ï¸âƒ£ Calcul de la matrice de covariance  
Nous calculons la matrice de covariance entre les variables. Elle montre comment elles varient ensemble.  

Exemple (valeurs fictives) :  
\[
\begin{bmatrix}
1.00 & 0.85 & 0.60 & 0.90 \\
0.85 & 1.00 & 0.55 & 0.88 \\
0.60 & 0.55 & 1.00 & 0.50 \\
0.90 & 0.88 & 0.50 & 1.00
\end{bmatrix}
\]
oÃ¹ :  
- 1.00 = une variable est parfaitement corrÃ©lÃ©e avec elle-mÃªme.  
- 0.85 entre "Revenu annuel" et "Score crÃ©dit" montre quâ€™ils sont fortement corrÃ©lÃ©s.  
- 0.50 entre "Age" et "Montant Ã©pargne" indique une corrÃ©lation plus faible.

 3ï¸âƒ£ Calcul des valeurs propres et vecteurs propres  
Nous trouvons les valeurs propres et vecteurs propres pour obtenir les nouvelles directions des composantes principales.  

Exemple (valeurs fictives) :  

| Composante principale | Valeur propre | Variance expliquÃ©e (%) |
|----------------------|--------------|----------------------|
| CP1                  | 2.8          | 70%                  |
| CP2                  | 1.0          | 25%                  |
| CP3                  | 0.15         | 4%                   |
| CP4                  | 0.05         | 1%                   |

On voit que les deux premiÃ¨res composantes expliquent 95% de la variance. On peut donc rÃ©duire nos 4 variables Ã  2 composantes.

 4ï¸âƒ£ Choix du nombre de composantes  
On utilise le critÃ¨re du coude (voir graphique ci-dessous).  

ğŸ”½ Graphique de la somme des variances expliquÃ©es  
_(Ajoutons une image pour illustrer le critÃ¨re du coude)_  

> _Une image montrant une courbe dÃ©croissante oÃ¹ la variance expliquÃ©e diminue fortement aprÃ¨s CP2, justifiant le choix de 2 composantes._

 5ï¸âƒ£ Projection des donnÃ©es sur les nouvelles composantes  
On transforme les anciennes variables en nouvelles coordonnÃ©es dans lâ€™espace des composantes principales.  

| Client | CP1  | CP2  |
|--------|------|------|
| A      | -1.2 | 0.8  |
| B      | -0.8 | 1.0  |
| C      | -1.8 | -0.5 |
| D      | 1.5  | -1.2 |
| E      | 2.2  | -1.5 |
| F      | -2.0 | -0.8 |
| G      | 1.2  | -0.9 |
| H      | -1.0 | 0.5  |
| I      | -0.5 | 1.2  |
| J      | 1.8  | -1.0 |

ğŸ”½ Graphique de projection des clients sur CP1 et CP2  
_(Ajoutons un scatter plot des clients dans le nouvel espace des composantes principales)_  

> _Une image montrant un graphe avec deux axes (CP1 et CP2), oÃ¹ les clients sont positionnÃ©s selon leurs nouvelles coordonnÃ©es._

---

 3. InterprÃ©tation des rÃ©sultats  

- CP1 (70% de la variance) : Oppose les clients avec un faible revenu, faible Ã©pargne, et score crÃ©dit bas (valeurs nÃ©gatives) aux clients riches, avec une forte Ã©pargne et un bon score crÃ©dit (valeurs positives).  
- CP2 (25% de la variance) : Semble Ãªtre influencÃ©e par lâ€™Ã¢ge et des comportements dâ€™Ã©pargne distincts.  

On peut maintenant visualiser et segmenter les clients facilement avec ces deux dimensions au lieu des 4 dâ€™origine.

---

 4. Conclusion  

âœ… RÃ©duction de dimension : De 4 Ã  2 variables tout en conservant 95% de lâ€™information.  
âœ… Visualisation plus facile des groupes de clients.  
âœ… Aide Ã  la segmentation pour du marketing bancaire (ex : clients jeunes & petits revenus vs. seniors & gros Ã©pargnants).  

---

1. Graphique du critÃ¨re du coude ğŸ“ˆ 

2. Scatter plot des clients dans lâ€™espace des composantes principales ğŸ”µğŸ”´  

==========================================================================================================================
